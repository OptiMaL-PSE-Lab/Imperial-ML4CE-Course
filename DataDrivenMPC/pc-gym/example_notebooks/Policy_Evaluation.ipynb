{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Policy Evaluation Notebook\n",
    "\n",
    "This notebook describes the policy evaluation tools available in `pc-gym`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcgym import make_env\n",
    "import numpy as np \n",
    "from stable_baselines3 import PPO, SAC\n",
    "from pcgym import reproducibility_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter required setpoints for each state.\n",
    "T = 26\n",
    "nsteps = 100\n",
    "SP = {\n",
    "    'Ca': [0.85 for i in range(int(nsteps/2))] + [0.9 for i in range(int(nsteps/2))],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous box action space\n",
    "action_space = {\n",
    "    'low': np.array([295]),\n",
    "    'high':np.array([302]) \n",
    "}\n",
    "\n",
    "# Continuous box observation space\n",
    "observation_space = {\n",
    "    'low' : np.array([0.7,300,0.8]),\n",
    "    'high' : np.array([1,350,0.9])  \n",
    "}\n",
    "\n",
    "r_scale ={\n",
    "    'Ca': 1e3 #Reward scale for each state\n",
    "}\n",
    "env_params = {\n",
    "    'N': nsteps, # Number of time steps\n",
    "    'tsim':T, # Simulation Time\n",
    "    'SP':SP, # Setpoint\n",
    "    'o_space' : observation_space, # Observation space\n",
    "    'a_space' : action_space, # Action space\n",
    "    'x0': np.array([0.8,330,0.8]), # Initial conditions \n",
    "    'model': 'cstr_ode', # Select the model\n",
    "    'r_scale': r_scale, # Scale the L1 norm used for reward (|x-x_sp|*r_scale)\n",
    "    'normalise_a': True, # Normalise the actions\n",
    "    'normalise_o':True, # Normalise the states,\n",
    "    'noise':True, # Add noise to the states\n",
    "    'integration_method': 'casadi', # Select the integration method\n",
    "    'noise_percentage':0.001 # Noise percentage\n",
    "}\n",
    "env = make_env(env_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -72.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 1385     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 100          |\n",
      "|    ep_rew_mean          | -95.2        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1116         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061045997 |\n",
      "|    clip_fraction        | 0.0573       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -0.0228      |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 22.1         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00378     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 120          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | -51.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 979         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007895479 |\n",
      "|    clip_fraction        | 0.0658      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 30.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0067     |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 80.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | -60.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 897         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011537282 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 33.8        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 75.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | -51.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 897         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007241652 |\n",
      "|    clip_fraction        | 0.0788      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 25          |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00399    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 64.5        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "nsteps_learning = 1e4\n",
    "PPO_policy = PPO('MlpPolicy', env, verbose=1,learning_rate=0.001).learn(nsteps_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -86.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 400      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.119    |\n",
      "|    critic_loss     | 0.107    |\n",
      "|    ent_coef        | 0.742    |\n",
      "|    ent_coef_loss   | -0.505   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 299      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -46.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 81       |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 800      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.261    |\n",
      "|    critic_loss     | 0.567    |\n",
      "|    ent_coef        | 0.503    |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 699      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -30.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 75       |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 1200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.387    |\n",
      "|    critic_loss     | 0.0556   |\n",
      "|    ent_coef        | 0.353    |\n",
      "|    ent_coef_loss   | -1.34    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1099     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "SAC_policy = SAC('MlpPolicy', env, verbose=1,learning_rate=0.001).learn(1.5e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows the policy and oracle comparison capability. The oracle is defined as a NMPC with perfect model (oracle uses ocp with control cost which differs from the reward function used by the RL agent) and can be sensitive to tuning of the horizon and control cost. A visualisation of the reward distribution can also be shown using `dist_reward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator, data = env.plot_rollout({'PPO': PPO_policy,'SAC':SAC_policy}, reps = 50, oracle=True, dist_reward=True,MPC_params={'N':15,'R':5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pc-gym` can also calculate dispersion and performance metrics of the distribution of rewards to allow the user to evaluate their policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalarised_performance {'oracle': {'r': array([801.51417093])}, 'PPO': {'r': array([3078.90847064])}, 'SAC': {'r': array([1493.78017083])}}\n"
     ]
    }
   ],
   "source": [
    "policy_measure = reproducibility_metric(dispersion='mad', performance='mean', scalarised_weight=2e3)\n",
    "scalarised_performance = policy_measure.evaluate(evaluator, component='r')\n",
    "\n",
    "print('scalarised_performance: ', scalarised_performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
